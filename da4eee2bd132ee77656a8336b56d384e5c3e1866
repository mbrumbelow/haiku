{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "85f4458e_8347908e",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2023-07-25T18:17:20Z",
      "side": 1,
      "message": "libuv test results, poll vs. kqueue: https://gist.github.com/waddlesplash/713c6a2d07e449320c11c7c44b02a0a4\n\nThe timeouts are the most concerning, of course. Not sure which of those should be investigated first...",
      "revId": "da4eee2bd132ee77656a8336b56d384e5c3e1866",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "6c7b9f6e_6c6c1019",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2023-07-25T18:53:46Z",
      "side": 1,
      "message": "Hmm, some failures are intermittent. fs_chmod, for example, succeeds some of the time and fails other times. It hangs waiting on mutex_sem_acquire syscall, and the other threads are all waiting on condition variables. This problem may be totally unrelated to kqueue, then.",
      "parentUuid": "85f4458e_8347908e",
      "revId": "da4eee2bd132ee77656a8336b56d384e5c3e1866",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "2eafe0ff_b3f09a5f",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1000023
      },
      "writtenOn": "2023-07-25T19:21:50Z",
      "side": 1,
      "message": "libuv tests on VM tend to be intermittent, indeed. It should be more reliable on real hardware. readable_on_eof looks like it should be investigated.",
      "parentUuid": "6c7b9f6e_6c6c1019",
      "revId": "da4eee2bd132ee77656a8336b56d384e5c3e1866",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d6d5f5ec_945e8198",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2023-07-25T20:30:39Z",
      "side": 1,
      "message": "A lot of the timeouts were due to a deadlock in our unnamed-sem code. Fixed in hrev57169. This reduces a lot of the differences (and fixed more tests, probably if I reran the poll tests they would be fixed there too.)\n\nI updated the gist. In this run readable_on_eof succeeded, but it still seems intermittent. Running it manually multiple times in a row has it fail with the assertion failure. shutdown_eof and others still fail outright.",
      "parentUuid": "2eafe0ff_b3f09a5f",
      "revId": "da4eee2bd132ee77656a8336b56d384e5c3e1866",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "618e2dd6_21413c15",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2023-07-26T00:14:07Z",
      "side": 1,
      "message": "Discovered that some tests intermittently KDLed. Tracked down the problems and fixed them in the event-queue commit. A few other adjustments made, test results updated: a few more regressions fixed. Most timeouts still remain however.",
      "parentUuid": "d6d5f5ec_945e8198",
      "revId": "da4eee2bd132ee77656a8336b56d384e5c3e1866",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9faa85b8_592876c6",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2023-07-26T03:33:24Z",
      "side": 1,
      "message": "As it turns out, our event_queue behaves as if EV_CLEAR was always specified (edge-triggered mode) while libuv never sets that (level-triggered mode.) Guess I will need to implement that...\n\nI wonder what the B_EVENT flag should be called? B_EVENT_LEVEL_TRIGGERED? B_EVENT_RESELECT_BEFORE_DEQUEUE (as that\u0027s functionally what it means, re-select\u0027ing any events that have occurred before dequeuing)?",
      "parentUuid": "618e2dd6_21413c15",
      "revId": "da4eee2bd132ee77656a8336b56d384e5c3e1866",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "44db8ce0_36b087d6",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2023-07-26T03:37:29Z",
      "side": 1,
      "message": "Well, it\u0027s more than just \"reselect before dequeue\" I suppose, as it also won\u0027t remove queued events after dequeue but only when the \"level\" changes. Furthermore we could rework the whole kernel to send select event *de*assertions instead of merely assertions, and that would allow this flag to be implemented without deselection/reselection. But that will be a lot of work, probably.",
      "parentUuid": "9faa85b8_592876c6",
      "revId": "da4eee2bd132ee77656a8336b56d384e5c3e1866",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    }
  ]
}