{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "f357cd4c_7d9b135b",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2025-02-28T17:08:34Z",
      "side": 1,
      "message": "This seems strange to me. Does NFSv4 provide no notifications at all when files are deleted? It looks like the NFSv4 driver doesn\u0027t implement the node monitoring interface (notify_entry_* and related methods), but there must be some way to do that, right? If it does, handling those messages and removing caches at the same time as sending node monitor messages sounds like a much better solution.\n\nFurthermore, there appears to be a \"cache time\" value. Does your server send that? It looks like the DirectoryCache does take it into account, so how does that come into play here?",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c15f4d77_f8abe40e",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000366
      },
      "writtenOn": "2025-03-01T15:10:53Z",
      "side": 1,
      "message": "I can\u0027t find anything in the NFSv4 RFC to indicate that NFS notifies clients when a file is deleted.  This is consistent with this line about NFS in Wikipedia:  \"changes made by one client are not immediately broadcast to other clients\" (https://en.wikipedia.org/wiki/Inotify).\n\nThe DirectoryCache refreshes automatically (every 5 seconds by default).  One effect of this is that \u0027ls\u0027 will not list a deleted file.  Should a check for stale nodes could be done every time the DirectoryCache is refreshed?  This would get rid of stale nodes sooner and more consistently, although maybe even then doing a additional check in Inode::CreateState/CreateObject would still be desirable to cover operations that happen right after a file was deleted by someone else, and before the DirectoryCache update got rid of the stale node.",
      "parentUuid": "f357cd4c_7d9b135b",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "923ffd38_1d13fbfa",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2025-03-01T16:22:16Z",
      "side": 1,
      "message": "Nodes being removed from the directory cache on refresh should also remove them from the entry cache, right? Wouldn\u0027t that solve this issue? The refresh time seems to come from the NFS server; it wouldn\u0027t make much sense for the server to reuse inode numbers within the refresh time window...\n\nI agree doing an additional check makes sense, but does it need to be as complicated as the HandleCollision method is here? Doing acquire_vnode will cause the VFS to try and fetch the vnode anew from the FS, which will probably cause RPC calls unnecessarily. If we have the vnode ID we should just be able to call remove_vnode() directly and this should just do nothing if the VFS does not know about the vnode, I think.",
      "parentUuid": "c15f4d77_f8abe40e",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2d7737e9_284ed198",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000366
      },
      "writtenOn": "2025-03-03T17:56:35Z",
      "side": 1,
      "message": "Currently, an entry is only removed from the entry cache if the file is deleted by the client.  If someone else deletes a file, the entry cache is not updated at the time of the DirectoryCache refresh.  There\u0027s no reason that DirectoryCache refresh code can\u0027t be updated to compare the new and old DirectoryCache contents and update the entry cache accordingly.  However, (and I was not thinking about this when I wrote my last reply) the DirectoryCache refresh is \u0027lazy.\u0027  After the cache expires, it will not refresh until prompted (for example, by nfs4_read_dir).  So even if I modify the DirectoryCache refresh to remove stale vnodes and entry cache entries, these actions will only happen when the client user runs ls or opens a Tracker window, not every x seconds.\n\nThe expiration time that I\u0027m looking at (DirectoryCache::fExpirationTime) seems to come from the client end rather than the server.  The DirectoryCache constructor sets fExpirationTime based on the MountConfiguration that is initialized in ParseArguments, which sets MountConfiguration::fDirectoryCacheTime to 5 seconds unless the client user provided a different value as a command line argument.  Given this and the lazy refresh method, I would expect the server to reuse an inode number even if the client\u0027s DirectoryCache still has that number assigned to a file.\n\nTo simplify HandleCollision, I don\u0027t think I can use remove_vnode() alone because it won\u0027t free a published node, even if the node has a ref count of 0 at the time.  It\u0027s the call to put_vnode(), after the node is marked removed, that actually triggers free_vnode() and nfs4_remove_vnode() (if the ref count is dropping to 0).  I thought that if the vnode is already in memory, then acquire_vnode() and get_vnode() would be efficient because they would just lookup and work with that existing node, but maybe I am wrong about that.",
      "parentUuid": "923ffd38_1d13fbfa",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "02ba7095_dde042d1",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2025-03-03T18:19:15Z",
      "side": 1,
      "message": "OK, I see that now re. expiration periods.\n\nIf the node isn\u0027t in memory, then get_vnode will cause the VFS to try and read it in, which isn\u0027t what we want here.\n\nMaybe we should not use the EntryCache here at all? It looks like nfs4_lookup() doesn\u0027t use the DirectoryCache, but maybe it could, just like read_dir does, and then that would get most of the performance benefit of the EntryCache with the expiration-refresh behavior.\n\nIt\u0027s not really possible to force a vnode to be freed, there may still be open FDs that point to it. All that can be done here is to mark it removed, and then to have the object inside NFS4 just return errors instead of trying to do anything. (Well, there\u0027s the \"disconnect vnodes\" feature of the VFS, but this is very expensive and only used on unmount currently. We could potentially extend it, but this is such a niche usecase that it\u0027s probably best for NFS to just handle it.)\n\nSo, the collision detection should just set some flag on the existing inode/etc. objects and \"orphan\" them. When the last FDs are closed, then the VFS will free the object.",
      "parentUuid": "2d7737e9_284ed198",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "5ed32da4_1c4d65e4",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000366
      },
      "writtenOn": "2025-03-04T17:29:26Z",
      "side": 1,
      "message": "I will try to revise nfs4_lookup() to make use of the DirectoryCache.\n\nIf our response to a collision is to mark a stale node invalid at the FS level, and mark it removed at the VFS level, I think when the FS calls get_vnode() to try to construct a new node with the same inode number, the VFS will just return a pointer to the stale node, since the VFS won\u0027t ignore a vnode just because it is marked removed.  It seems like the stale node needs to be freed at the VFS level in order to make way for the new one.\n\nI noticed that if I delete a test file at the server end, then create a new file on the client, the server consistently assigns the inode number of the deleted file to the new file.  However, if I do the same thing, except that I open the file to be deleted on the client end before deleting it, the server won\u0027t reuse that inode number until after the client closes it.  I interpret this to mean that if we do detect a collision, then we know that there are no open FDs to the deleted file, and can rely on the vnode being freed when HandleCollision drops its last ref because at that point the node\u0027s ref count will go to 0.  Since this FS seems to internally track whether this client has a file open (with Inode::fOpenState) I could probably add an inexpensive assert to HandleCollision that confirms the assumption that there are no open FDs.\n\nI am hoping that by conditionally calling get_vnode only if aqcuire_vnode succeeds,  HandleCollision will never cause a vnode to be constructed.  If acquire_vnode succeeds, doesn\u0027t that tell us the vnode is already in memory?\n\nInstead of freeing the vnode, it might be possible to just swap out the private node.  This is what the driver\u0027s get_new_vnode() seems to be intended to do.  But I don\u0027t think this will work in all cases.  For example, if the inode of a directory is reused and assigned to a regular file, the VFS will continue to see that vnode\u0027s file type as S_IFDIR, which will cause file_write() to return an error.",
      "parentUuid": "02ba7095_dde042d1",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c371253e_cbfb4e82",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2025-03-04T17:41:43Z",
      "side": 1,
      "message": "\u003e It seems like the stale node needs to be freed at the VFS level in order to make way for the new one.\n\nYes, but if there are no open FDs, then remove_vnode + put_vnode should take care of freeing it.\n\n\u003e  If acquire_vnode succeeds, doesn\u0027t that tell us the vnode is already in memory?\n\nAh, yes it does. Indeed that\u0027s a different behavior than get_vnode() has. So your logic may be basically fine as-is then.",
      "parentUuid": "5ed32da4_1c4d65e4",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "df003bd3_e15db0b1",
        "filename": "src/add-ons/kernel/file_systems/nfs4/FileSystem.cpp",
        "patchSetId": 1
      },
      "lineNbr": 147,
      "author": {
        "id": 1000630
      },
      "writtenOn": "2025-02-28T16:20:47Z",
      "side": 1,
      "message": "Suggestion from `haiku-format` (change):\n```c++\nFileSystem::Mount(FileSystem** _fs, RPC::Server* serv, const char* serverName, const char* fsPath,\n\tfs_volume* volume, const MountConfiguration\u0026 configuration)\n```",
      "range": {
        "startLine": 146,
        "startChar": 0,
        "endLine": 147,
        "endChar": 0
      },
      "tag": "autogenerated:experimental-formatting-bot",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9d4a92e2_1efee0cc",
        "filename": "src/add-ons/kernel/file_systems/nfs4/FileSystem.cpp",
        "patchSetId": 1
      },
      "lineNbr": 489,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2025-03-04T17:41:43Z",
      "side": 1,
      "message": "This function probably should never fail, based on how it\u0027s used. The failure paths should just be ASSERT or ASSERT_ALWAYS (rather than having the ASSERTs elsewhere)",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "be414556_7bdf61e4",
        "filename": "src/add-ons/kernel/file_systems/nfs4/FileSystem.cpp",
        "patchSetId": 1
      },
      "lineNbr": 489,
      "author": {
        "id": 1000366
      },
      "writtenOn": "2025-03-06T19:16:02Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "9d4a92e2_1efee0cc",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "5ed15722_d44a1e64",
        "filename": "src/add-ons/kernel/file_systems/nfs4/FileSystem.cpp",
        "patchSetId": 1
      },
      "lineNbr": 490,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2025-03-04T17:41:43Z",
      "side": 1,
      "message": "Not sure about the name. Since it\u0027s called even when there\u0027s no collision and exists to ensure there aren\u0027t collisions, maybe `EnsureNoCollision(ino_t newID,...` would be a better name?",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "0969f24b_c3442ccf",
        "filename": "src/add-ons/kernel/file_systems/nfs4/FileSystem.cpp",
        "patchSetId": 1
      },
      "lineNbr": 490,
      "author": {
        "id": 1000366
      },
      "writtenOn": "2025-03-06T19:16:02Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "5ed15722_d44a1e64",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e013a54f_159bdaaa",
        "filename": "src/add-ons/kernel/file_systems/nfs4/FileSystem.cpp",
        "patchSetId": 1
      },
      "lineNbr": 518,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2025-03-04T17:41:43Z",
      "side": 1,
      "message": "What happens if there is an open request in progress for this vnode, at the time this function is called? That seems to be the one case where it wouldn\u0027t be gone here. Maybe with the removal of the entry cache and the switch to a new DirectoryCache strategy, that\u0027s less likely, but it still seems like it could happen.",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ba1aea8c_5caf2695",
        "filename": "src/add-ons/kernel/file_systems/nfs4/FileSystem.cpp",
        "patchSetId": 1
      },
      "lineNbr": 518,
      "author": {
        "id": 1000366
      },
      "writtenOn": "2025-03-06T19:16:02Z",
      "side": 1,
      "message": "I think we can rely on that not happening.  My reasoning is:  if the ID was already recycled by the server at the time EnsureNoCollision is called, then we know the stale file was already deleted from the server and that there was a point in time after that deletion when we had no open FDs to it (which made the ID eligible for recycling).  If someone tries to open the removed file after that point, open_vnode() should fail when it calls nfs4_open, which (when there are no pre-existing open FDs, which is true in this situation) will always check the server and find that the file with the stale handle isn\u0027t there anymore.",
      "parentUuid": "e013a54f_159bdaaa",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "867181dc_d72bb7be",
        "filename": "src/add-ons/kernel/file_systems/nfs4/Inode.h",
        "patchSetId": 1
      },
      "lineNbr": 123,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2025-03-04T17:41:43Z",
      "side": 1,
      "message": "probably should just be called `IsStale`",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "cee10c01_0326996e",
        "filename": "src/add-ons/kernel/file_systems/nfs4/Inode.h",
        "patchSetId": 1
      },
      "lineNbr": 123,
      "author": {
        "id": 1000366
      },
      "writtenOn": "2025-03-06T19:16:02Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "867181dc_d72bb7be",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "a5ab7f52_2f219bed",
        "filename": "src/add-ons/kernel/file_systems/nfs4/kernel_interface.cpp",
        "patchSetId": 1
      },
      "lineNbr": 197,
      "author": {
        "id": 1000630
      },
      "writtenOn": "2025-02-28T16:20:47Z",
      "side": 1,
      "message": "Suggestion from `haiku-format` (change):\n```c++\n\tresult \u003d FileSystem::Mount(\u0026fs, server, serverName, path, volume, config);\n```",
      "range": {
        "startLine": 196,
        "startChar": 0,
        "endLine": 197,
        "endChar": 0
      },
      "tag": "autogenerated:experimental-formatting-bot",
      "revId": "6d5143954e8ad66472397d56dffbc7037fd30786",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    }
  ]
}