{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "d534cd67_cd59dff0",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000172
      },
      "writtenOn": "2023-09-25T18:44:42Z",
      "side": 1,
      "message": "Experimental. Do not merge.",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "306ea59d_47c80f77",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2023-09-25T18:48:13Z",
      "side": 1,
      "message": "I think mimalloc is similar to other \"recent\" allocators in how it allocates memory (i.e. expects overcommitting?)\n\nWe should take a look at Hoard3.",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "1ee8ba16_7dba63d2",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000172
      },
      "writtenOn": "2023-09-25T19:00:41Z",
      "side": 1,
      "message": "Hoard3 is the same in this aspect.\n\nCurrently used hoard2 seems have x86 default atomic memory access expectations that don\u0027t work for riscv64 or arm[64], causing random malloc() failures.",
      "parentUuid": "306ea59d_47c80f77",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "4e4c4b3b_51c3c9f2",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000001
      },
      "writtenOn": "2023-09-25T19:10:00Z",
      "side": 1,
      "message": "It would also maybe be useful to document our past findings about allocators so people don\u0027t have to \"rediscover\" them.\n\nHere\u0027s what I remember:\n\nWe have tried rpmalloc (back in 2018, starting at hrev53136) and Waddlesplash also experimented with musl mallocng in 2020 but that never got merged.\n\nWith rpmalloc we had found at least the following problems:\n\n- It did not support very large alignments (such as allocations aligned to B_PAGE_SIZE). The author added them on our request.\n- It had memory fragmentation problems. Where hoard2 creates a single large area upfront and uses it, rpmalloc starts with a smaller area and allocates more small areas when there are new allocations. This results in a fragmented memory space, and, especially on 32bit systems, after a while, your app has a lot of small areas all over its address space, and not enough contiguous memory space to fit a large mmap or create_area\n- rpmalloc also wasted a lot of memory, that was reserved by the apps but not actually allocated. It seems it relied on the OS not actually mapping the RAM immediately, and filling it later as needed. Which is doable, but rpmalloc did not do anything special to request that from the OS, since on Linux, that seems to be the default way to allocate memory. In our case, explicit management would be needed, as well as error handling if the OS runs out of physical memory to allocate.\n- Creating many areas also wasn\u0027t helping performance, since managing a lot of areas was slow (maybe that improved a bit since then). It turns out reserving a large initial memory space at the start, and mapping physical pages to it as needed, ends up being simpler. As a sidenote, having many areas for malloc for each app in listareas and other tools also was not very convenient for debugging\n\nMaybe some of these problems are specific to rpmalloc.\n\nIt also raises some questions on underlying features: how we manage the virtual address space of applications (do we have a specific allocation strategy here? There is room for something to be done to have an efficient allocator that also does not fragment the memory space too much, for example a look at TLSF malloc would be interesting here, it is designed to handle such things on systems without an MMU where fragmentation is more critical). How does our rather aggressive ASLR come into play in fragmenting the address space, especially on 32bit systems?\n\nIn any case, an experiment like this will need a lot of testing on various configurations, to make sure it handles normal situations, low memory situations, and all special cases that can show up (apps needing a large non-malloc address space, for example). That\u0027s, of course, in addition to performance tests in terms of memory use and speed, which we need to run ourselves with our own usecases (experience shows that the benchmarks done by the allocator developers are always biased towards their own specific usecases).\n\nThat being said, it is a good idea to experiment with it, and the Jamfile is built in a way that makes it easy to have multiple allocators available and an easy switch between them at compile time. So, that\u0027s a good way to start all the needed experimentations :)\n\nIf the concern is for non-x86 architectures, it is also possible to use different allocators on different architectures?",
      "parentUuid": "306ea59d_47c80f77",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "8728961d_c1543dcd",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2023-09-25T19:18:27Z",
      "side": 1,
      "message": "\u003e Hoard3 is the same in this aspect.\n\nI don\u0027t think so. By default it allocates 2MB areas, yes, but this was a recent change, you can see it used to be 64KB. We can go back to that and it should be fine.\n\n\u003e Creating many areas also wasn\u0027t helping performance, since managing a lot of areas was slow (maybe that improved a bit since then). \n\nmmlr refactored VMUserAddressSpace to fix this, yes.",
      "parentUuid": "4e4c4b3b_51c3c9f2",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "1de51015_8fc3b28a",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000172
      },
      "writtenOn": "2023-09-26T21:27:22Z",
      "side": 1,
      "message": "No random malloc() failures on riscv64 for now, but need more testing.",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "91661575_bbcc6b4e",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000001
      },
      "writtenOn": "2023-09-28T07:12:43Z",
      "side": 1,
      "message": "From the things to test, git is interesting (on large repos, for example haiku or webkit), it does a lot of large mmap and this didn\u0027t work so well with rpmalloc.",
      "parentUuid": "1de51015_8fc3b28a",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8e53b60d_e6278240",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000016
      },
      "writtenOn": "2023-09-28T09:59:11Z",
      "side": 1,
      "message": "We do support reserving memory space. This allows allocators to start with a rather small area that it can later extend (without actually creating a new area, and) without having to worry that there is no space left early (even with ASLR).",
      "parentUuid": "8728961d_c1543dcd",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "6629244d_922b3254",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000052
      },
      "writtenOn": "2023-09-30T05:21:37Z",
      "side": 1,
      "message": "I should also add that Jessica had inquired into the usage of mimalloc as a system allocator: https://github.com/microsoft/mimalloc/issues/648\n\nThis could likely help provide answers into mimalloc\u0027s usage.",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "1d2f334f_436dfd59",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2023-10-07T02:01:09Z",
      "side": 1,
      "message": "The allocator hasn\u0027t seen much development for a while, though. Whether or how it\u0027s used in any of the featured applications isn\u0027t clear from some searches either.\n\nPart of the reason I had tried to go with musl\u0027s allocator was that it\u0027s battle-tested in multiple Linux distributions. These other allocators which aren\u0027t so tested can vary in quality and are often tuned to more specific workloads...",
      "parentUuid": "6629244d_922b3254",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ed553e46_4c03b054",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000001
      },
      "writtenOn": "2024-12-23T10:10:36Z",
      "side": 1,
      "message": "This discussion has been going on for a while over IRC, I think I don\u0027t understand Waddlesplash\u0027s point of view here.\n\nEvery workload is specific, and malloc tries to be a \"one size fits all\" generic API to cover all of them. We know this, and in some places in Haiku we use dedicated object pools instead for example.\n\nIt is hard to judge an allocator by someone else\u0027s benchmarks, or even by a design review. The only thing we can do is run our own tests and see how it goes. It doesn\u0027t matter if the allocator has been \"battle tested\" by someone else, because they likely have different needs and expectations.\n\nSo I see no reason to be opposed in principle to mimalloc. Let\u0027s test it and see how it goes.\n\nThat means we have at least the following candidates:\n\n- mimalloc (this patch)\n- openbsd malloc (another patch work in progress)\n- rpmalloc (previous attempt, but maybe the integration can be improved to reserve address space upfront and use a single growable area instead of many smaller ones)\n- hoard2 (current implementation), to provide a baseline\n- hoard2 with fixes (waddlesplash mentionned that he identified various problems with how hoard2 is integrated)\n- musl malloc\n- hoard3\n\nAnd things to test:\n\n- Behavior and speed in normal situations\n- In low memory situations\n- On non-x86 machines\n- On 32 bit systems\n- On various workloads: compiler benchmark (many processes with fe\u003c threads), on WebKit (large app but mostly single threaded), on git with big repositories (identified to be problematic with rpmalloc), general GUI app usage\n\nWithout all this being done, I don\u0027t see how we can decide that an allocator is suitable or not.",
      "parentUuid": "1d2f334f_436dfd59",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2ee5be49_4fcad901",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2024-12-23T17:06:02Z",
      "side": 1,
      "message": "\u003e It doesn\u0027t matter if the allocator has been \"battle tested\" by someone else, because they likely have different needs and expectations.\n\nMaybe, but an allocator used as a libc malloc implementation will have been \"battle tested\" in many of the same workloads we will use it in, even if we have a lot more cross-thread malloc usage than most. An allocator not ever used in such a situation, or only done so briefly, may not even be tuned for GUI workloads in the first place.\n\nOpenBSD developers really run OpenBSD with a GUI, and let Firefox use the allocator as its malloc (same as our port does at the moment), I believe. They also do not have any overcommit at all, and support and run on many non-x86 architectures and still support 32-bit, and all the workloads you specify here. Whereas `mimalloc` has only been used in some of those situations at best, and we\u0027d be testing it ourselves in the rest.\n\n\u003e On 32 bit systems\n\nSupposedly mimalloc supports this. However, when I went digging, I found this in `mold`\u0027s CMakeLists: https://github.com/rui314/mold/blob/main/CMakeLists.txt#L179\n\nEven if this has been fixed since the comment was added (though I can\u0027t find any evidence that it has), it probably means that *nobody* runs mimalloc \"in production\" on 32-bit systems at the moment. Furthermore, I actually don\u0027t know who even uses mimalloc \"in production\" at all besides `mold`: its README claims Bing, Azure, Unreal Engine, etc., but the Bing/Azure links just go to their homepages, and the Unreal Engine link goes to their documentation but is a 404.",
      "parentUuid": "ed553e46_4c03b054",
      "revId": "990ccb14580924c8454e42feaf48b29198fa20e5",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    }
  ]
}