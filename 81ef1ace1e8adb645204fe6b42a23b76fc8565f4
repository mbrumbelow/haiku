{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "4696e5b6_7118a05f",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1000011
      },
      "writtenOn": "2024-09-17T00:23:26Z",
      "side": 1,
      "message": "Top of syscall statistics for jam -q ... with hoard2:\n```\nTime % Usecs      Calls   Usecs/call Syscall\n------ ---------- ------- ---------- --------------------\n 36.20      66529    3871         17 read_dir\n 30.83      56664    9366          6 read_stat\n 24.51      45040    9828          4 open_dir\n  2.83       5199     523          9 write\n  2.47       4532    1669          2 resize_area\n  1.46       2688    1933          1 close\n  0.94       1724     535          3 read\n  0.44        801      13         61 map_file\n  0.21        391      14         27 open\n```\nand with OpenBSD malloc:\n```\nTime % Usecs      Calls   Usecs/call Syscall\n------ ---------- ------- ---------- --------------------\n 33.97      64451    3871         16 read_dir\n 31.53      59837    9366          6 read_stat\n 24.00      45541    9828          4 open_dir\n  2.58       4905     523          9 write\n  2.45       4640     113         41 unmap_memory\n  2.24       4245     698          6 create_area\n  1.45       2759    1933          1 close\n  1.07       2030     535          3 read\n  0.41        774      13         59 map_file\n  0.21        399      14         28 open\n```\nAn additional 4ms for unmap_memory isn\u0027t much. Meanwhile the create_area calls are more expensive but we have fewer of them than resize_area so that works out anyway. The extra time must be all CPU time I guess? Jam is single-threaded so there should be no mutex contention.",
      "revId": "81ef1ace1e8adb645204fe6b42a23b76fc8565f4",
      "serverId": "40b9299a-d8a8-485d-9b01-e6d3f45eefb5"
    }
  ]
}